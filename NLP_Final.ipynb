{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sparambil/github-first/blob/master/NLP_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJOBqN1XzGyg",
        "outputId": "a9ccaece-2341-4550-a26f-0ed76f921b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'fp-dataset-artifacts'...\n",
            "Host key verification failed.\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ]
        }
      ],
      "source": [
        "!git clone git@github.com:gregdurrett/fp-dataset-artifacts.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjtjwB0izd17",
        "outputId": "141c0c95-daa5-458c-a07d-2c1ab8f09ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUQdEwf5zf28",
        "outputId": "a3b1e97b-84a7-4b66-ad32-427ebb89b81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.46.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.26.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (0.20.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->-r requirements.txt (line 2)) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koPYnJSB0XLg",
        "outputId": "134586fb-a0d9-47d0-be0c-da847002e22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0qbz0gm0dKw",
        "outputId": "4a5f3e25-de41-4638-fadb-d8858ed32402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 00:34:16.883402: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 00:34:16.902760: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 00:34:16.908693: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 00:34:16.923487: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 00:34:17.914736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Create a W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Create an account here: https://wandb.ai/authorize?signup=true\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_003544-bmuhw63k\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./trained_model/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/bmuhw63k\u001b[0m\n",
            "{'loss': 0.6115, 'grad_norm': 4.03678560256958, 'learning_rate': 4.611680646163405e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4683, 'grad_norm': 3.8778998851776123, 'learning_rate': 4.2233612923268096e-05, 'epoch': 0.47}\n",
            "{'loss': 0.438, 'grad_norm': 2.337303400039673, 'learning_rate': 3.835041938490215e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4191, 'grad_norm': 3.1146271228790283, 'learning_rate': 3.4467225846536196e-05, 'epoch': 0.93}\n",
            "{'loss': 0.3884, 'grad_norm': 2.187290906906128, 'learning_rate': 3.058403230817024e-05, 'epoch': 1.16}\n",
            "{'loss': 0.3736, 'grad_norm': 2.215346574783325, 'learning_rate': 2.670083876980429e-05, 'epoch': 1.4}\n",
            "{'loss': 0.3667, 'grad_norm': 1.8506059646606445, 'learning_rate': 2.2817645231438335e-05, 'epoch': 1.63}\n",
            "{'loss': 0.3637, 'grad_norm': 2.8981740474700928, 'learning_rate': 1.8934451693072382e-05, 'epoch': 1.86}\n",
            "{'loss': 0.3464, 'grad_norm': 2.2675955295562744, 'learning_rate': 1.5051258154706432e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3374, 'grad_norm': 2.4657084941864014, 'learning_rate': 1.1168064616340479e-05, 'epoch': 2.33}\n",
            "{'loss': 0.3328, 'grad_norm': 2.365074872970581, 'learning_rate': 7.284871077974527e-06, 'epoch': 2.56}\n",
            "{'loss': 0.3311, 'grad_norm': 2.7011356353759766, 'learning_rate': 3.401677539608574e-06, 'epoch': 2.8}\n",
            "{'train_runtime': 4953.2066, 'train_samples_per_second': 332.734, 'train_steps_per_second': 1.3, 'train_loss': 0.39365368001984674, 'epoch': 3.0}\n",
            "100% 6438/6438 [1:21:19<00:00,  1.32it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./trained_model/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/bmuhw63k\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_003544-bmuhw63k/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --per_device_train_batch_size 256 --output_dir ./trained_model/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOAiLGAcY5JM",
        "outputId": "0fbdbcb7-8d58-490b-c216-443e6ad0d5e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 01:58:56.092846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 01:58:56.113732: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 01:58:56.119879: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 01:58:56.135746: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 01:58:57.117626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Map (num_proc=2): 100% 9842/9842 [00:00<00:00, 9939.47 examples/s] \n",
            "/content/run.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            " 99% 1224/1231 [00:15<00:00, 78.24it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_015925-ns36637m\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./eval_output/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/ns36637m\u001b[0m\n",
            "100% 1231/1231 [00:16<00:00, 72.60it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.3074508011341095, 'eval_model_preparation_time': 0.0032, 'eval_accuracy': 0.8913838863372803, 'eval_runtime': 16.1419, 'eval_samples_per_second': 609.717, 'eval_steps_per_second': 76.261}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./eval_output/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/ns36637m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_015925-ns36637m/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B9cCFuXDtXdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --per_device_train_batch_size 128 --output_dir ./trained_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIPxwVW7-45q",
        "outputId": "8fe19e50-e476-49b6-d942-b9dae449b52d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 02:03:09.446450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 02:03:09.465939: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 02:03:09.471926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 02:03:09.486008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 02:03:10.513490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_020323-8vfsvmp4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./trained_model/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/8vfsvmp4\u001b[0m\n",
            "{'loss': 0.6723, 'grad_norm': 3.344348669052124, 'learning_rate': 4.805840323081703e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5009, 'grad_norm': 3.843803644180298, 'learning_rate': 4.611680646163405e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4619, 'grad_norm': 3.029366970062256, 'learning_rate': 4.4175209692451076e-05, 'epoch': 0.35}\n",
            "{'loss': 0.4418, 'grad_norm': 3.081723928451538, 'learning_rate': 4.2233612923268096e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4332, 'grad_norm': 2.6291940212249756, 'learning_rate': 4.029201615408512e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4177, 'grad_norm': 3.741858959197998, 'learning_rate': 3.835041938490215e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4078, 'grad_norm': 3.013871431350708, 'learning_rate': 3.640882261571917e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4018, 'grad_norm': 3.1948719024658203, 'learning_rate': 3.4467225846536196e-05, 'epoch': 0.93}\n",
            "{'loss': 0.3806, 'grad_norm': 3.0223143100738525, 'learning_rate': 3.2525629077353216e-05, 'epoch': 1.05}\n",
            "{'loss': 0.3629, 'grad_norm': 3.1168994903564453, 'learning_rate': 3.058403230817024e-05, 'epoch': 1.16}\n",
            "{'loss': 0.3596, 'grad_norm': 3.1013693809509277, 'learning_rate': 2.8642435538987266e-05, 'epoch': 1.28}\n",
            "{'loss': 0.3568, 'grad_norm': 2.4332268238067627, 'learning_rate': 2.670083876980429e-05, 'epoch': 1.4}\n",
            "{'loss': 0.3533, 'grad_norm': 3.677039861679077, 'learning_rate': 2.4759242000621312e-05, 'epoch': 1.51}\n",
            "{'loss': 0.3521, 'grad_norm': 2.9702353477478027, 'learning_rate': 2.2817645231438335e-05, 'epoch': 1.63}\n",
            "{'loss': 0.352, 'grad_norm': 3.0611441135406494, 'learning_rate': 2.087604846225536e-05, 'epoch': 1.75}\n",
            "{'loss': 0.3444, 'grad_norm': 2.52557373046875, 'learning_rate': 1.8934451693072382e-05, 'epoch': 1.86}\n",
            "{'loss': 0.3404, 'grad_norm': 3.501307725906372, 'learning_rate': 1.6992854923889405e-05, 'epoch': 1.98}\n",
            "{'loss': 0.3142, 'grad_norm': 3.100332260131836, 'learning_rate': 1.5051258154706432e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3167, 'grad_norm': 3.675123691558838, 'learning_rate': 1.3109661385523455e-05, 'epoch': 2.21}\n",
            "{'loss': 0.3177, 'grad_norm': 3.090409755706787, 'learning_rate': 1.1168064616340479e-05, 'epoch': 2.33}\n",
            "{'loss': 0.3121, 'grad_norm': 3.5972626209259033, 'learning_rate': 9.226467847157502e-06, 'epoch': 2.45}\n",
            "{'loss': 0.3102, 'grad_norm': 2.64577579498291, 'learning_rate': 7.284871077974527e-06, 'epoch': 2.56}\n",
            "{'loss': 0.3141, 'grad_norm': 2.7732391357421875, 'learning_rate': 5.34327430879155e-06, 'epoch': 2.68}\n",
            "{'loss': 0.3088, 'grad_norm': 2.5375711917877197, 'learning_rate': 3.401677539608574e-06, 'epoch': 2.8}\n",
            "{'loss': 0.3129, 'grad_norm': 3.141472816467285, 'learning_rate': 1.4600807704255982e-06, 'epoch': 2.91}\n",
            "{'train_runtime': 4978.95, 'train_samples_per_second': 331.014, 'train_steps_per_second': 2.586, 'train_loss': 0.37601167750528935, 'epoch': 3.0}\n",
            "100% 12876/12876 [1:22:57<00:00,  2.59it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./trained_model/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/8vfsvmp4\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_020323-8vfsvmp4/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQMVmJMitgGq",
        "outputId": "2f22c07f-164d-4320-a1be-d200ab9a0d20"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 03:27:27.115561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 03:27:27.136310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 03:27:27.142552: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 03:27:27.158620: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 03:27:28.220697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "100% 1229/1231 [00:16<00:00, 70.97it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_032757-stezss6i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./eval_output/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/stezss6i\u001b[0m\n",
            "100% 1231/1231 [00:18<00:00, 66.98it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.2987252473831177, 'eval_model_preparation_time': 0.0033, 'eval_accuracy': 0.8941271901130676, 'eval_runtime': 17.5141, 'eval_samples_per_second': 561.947, 'eval_steps_per_second': 70.286}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./eval_output/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/stezss6i\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_032757-stezss6i/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --per_device_train_batch_size 64 --output_dir ./trained_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaC3Jd8xupu5",
        "outputId": "6ee653d7-d32d-4d47-ef1e-81b6e29639c0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 03:33:51.972246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 03:33:51.993423: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 03:33:52.000255: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 03:33:52.015402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 03:33:53.087562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_033405-yqwlbq1b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./trained_model/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/yqwlbq1b\u001b[0m\n",
            "{'loss': 0.6982, 'grad_norm': 5.539941310882568, 'learning_rate': 4.902920161540851e-05, 'epoch': 0.06}\n",
            "{'loss': 0.5361, 'grad_norm': 6.230140209197998, 'learning_rate': 4.805840323081703e-05, 'epoch': 0.12}\n",
            "{'loss': 0.4989, 'grad_norm': 5.580577850341797, 'learning_rate': 4.708760484622554e-05, 'epoch': 0.17}\n",
            "{'loss': 0.4754, 'grad_norm': 4.630081653594971, 'learning_rate': 4.611680646163405e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4599, 'grad_norm': 3.5349879264831543, 'learning_rate': 4.514600807704256e-05, 'epoch': 0.29}\n",
            "{'loss': 0.447, 'grad_norm': 4.759328842163086, 'learning_rate': 4.4175209692451076e-05, 'epoch': 0.35}\n",
            "{'loss': 0.4373, 'grad_norm': 4.02966833114624, 'learning_rate': 4.3204411307859586e-05, 'epoch': 0.41}\n",
            "{'loss': 0.4332, 'grad_norm': 4.712879180908203, 'learning_rate': 4.2233612923268096e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4228, 'grad_norm': 6.13850736618042, 'learning_rate': 4.1262814538676606e-05, 'epoch': 0.52}\n",
            "{'loss': 0.428, 'grad_norm': 4.080923557281494, 'learning_rate': 4.029201615408512e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4139, 'grad_norm': 3.6321184635162354, 'learning_rate': 3.932121776949363e-05, 'epoch': 0.64}\n",
            "{'loss': 0.4086, 'grad_norm': 3.541447639465332, 'learning_rate': 3.835041938490215e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4101, 'grad_norm': 4.000677585601807, 'learning_rate': 3.737962100031065e-05, 'epoch': 0.76}\n",
            "{'loss': 0.3922, 'grad_norm': 3.60536527633667, 'learning_rate': 3.640882261571917e-05, 'epoch': 0.82}\n",
            "{'loss': 0.3988, 'grad_norm': 3.935723304748535, 'learning_rate': 3.543802423112768e-05, 'epoch': 0.87}\n",
            "{'loss': 0.3936, 'grad_norm': 4.089642524719238, 'learning_rate': 3.4467225846536196e-05, 'epoch': 0.93}\n",
            "{'loss': 0.3894, 'grad_norm': 5.721362113952637, 'learning_rate': 3.3496427461944706e-05, 'epoch': 0.99}\n",
            "{'loss': 0.3584, 'grad_norm': 4.38346529006958, 'learning_rate': 3.2525629077353216e-05, 'epoch': 1.05}\n",
            "{'loss': 0.3524, 'grad_norm': 3.4548630714416504, 'learning_rate': 3.1554830692761726e-05, 'epoch': 1.11}\n",
            "{'loss': 0.3566, 'grad_norm': 4.693231582641602, 'learning_rate': 3.058403230817024e-05, 'epoch': 1.16}\n",
            "{'loss': 0.351, 'grad_norm': 5.690192222595215, 'learning_rate': 2.9613233923578752e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3486, 'grad_norm': 4.3764519691467285, 'learning_rate': 2.8642435538987266e-05, 'epoch': 1.28}\n",
            "{'loss': 0.3505, 'grad_norm': 3.9853618144989014, 'learning_rate': 2.7671637154395776e-05, 'epoch': 1.34}\n",
            "{'loss': 0.3459, 'grad_norm': 4.313938617706299, 'learning_rate': 2.670083876980429e-05, 'epoch': 1.4}\n",
            "{'loss': 0.3434, 'grad_norm': 5.449315547943115, 'learning_rate': 2.57300403852128e-05, 'epoch': 1.46}\n",
            "{'loss': 0.3414, 'grad_norm': 5.798837184906006, 'learning_rate': 2.4759242000621312e-05, 'epoch': 1.51}\n",
            "{'loss': 0.3452, 'grad_norm': 4.128596305847168, 'learning_rate': 2.3788443616029822e-05, 'epoch': 1.57}\n",
            "{'loss': 0.3429, 'grad_norm': 4.237811088562012, 'learning_rate': 2.2817645231438335e-05, 'epoch': 1.63}\n",
            "{'loss': 0.3447, 'grad_norm': 5.017079830169678, 'learning_rate': 2.1846846846846845e-05, 'epoch': 1.69}\n",
            "{'loss': 0.3395, 'grad_norm': 3.9179060459136963, 'learning_rate': 2.087604846225536e-05, 'epoch': 1.75}\n",
            "{'loss': 0.3393, 'grad_norm': 4.8166632652282715, 'learning_rate': 1.9905250077663872e-05, 'epoch': 1.81}\n",
            "{'loss': 0.3336, 'grad_norm': 4.999579429626465, 'learning_rate': 1.8934451693072382e-05, 'epoch': 1.86}\n",
            "{'loss': 0.3329, 'grad_norm': 5.907031059265137, 'learning_rate': 1.7963653308480895e-05, 'epoch': 1.92}\n",
            "{'loss': 0.3329, 'grad_norm': 4.7226643562316895, 'learning_rate': 1.6992854923889405e-05, 'epoch': 1.98}\n",
            "{'loss': 0.307, 'grad_norm': 5.963385581970215, 'learning_rate': 1.602205653929792e-05, 'epoch': 2.04}\n",
            "{'loss': 0.2916, 'grad_norm': 5.662782192230225, 'learning_rate': 1.5051258154706432e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3073, 'grad_norm': 5.0550456047058105, 'learning_rate': 1.4080459770114942e-05, 'epoch': 2.16}\n",
            "{'loss': 0.2969, 'grad_norm': 4.651728630065918, 'learning_rate': 1.3109661385523455e-05, 'epoch': 2.21}\n",
            "{'loss': 0.3054, 'grad_norm': 6.425514221191406, 'learning_rate': 1.2138863000931967e-05, 'epoch': 2.27}\n",
            "{'loss': 0.2993, 'grad_norm': 4.2093963623046875, 'learning_rate': 1.1168064616340479e-05, 'epoch': 2.33}\n",
            "{'loss': 0.2991, 'grad_norm': 4.852103233337402, 'learning_rate': 1.019726623174899e-05, 'epoch': 2.39}\n",
            "{'loss': 0.2965, 'grad_norm': 4.373034477233887, 'learning_rate': 9.226467847157502e-06, 'epoch': 2.45}\n",
            "{'loss': 0.2912, 'grad_norm': 3.6251063346862793, 'learning_rate': 8.255669462566015e-06, 'epoch': 2.5}\n",
            "{'loss': 0.2978, 'grad_norm': 3.515770196914673, 'learning_rate': 7.284871077974527e-06, 'epoch': 2.56}\n",
            "{'loss': 0.2955, 'grad_norm': 5.599238872528076, 'learning_rate': 6.314072693383039e-06, 'epoch': 2.62}\n",
            "{'loss': 0.2971, 'grad_norm': 2.249180316925049, 'learning_rate': 5.34327430879155e-06, 'epoch': 2.68}\n",
            "{'loss': 0.2917, 'grad_norm': 4.806398391723633, 'learning_rate': 4.372475924200063e-06, 'epoch': 2.74}\n",
            "{'loss': 0.2942, 'grad_norm': 4.977540493011475, 'learning_rate': 3.401677539608574e-06, 'epoch': 2.8}\n",
            "{'loss': 0.2997, 'grad_norm': 4.115437030792236, 'learning_rate': 2.430879155017086e-06, 'epoch': 2.85}\n",
            "{'loss': 0.2958, 'grad_norm': 3.7217893600463867, 'learning_rate': 1.4600807704255982e-06, 'epoch': 2.91}\n",
            "{'loss': 0.2955, 'grad_norm': 5.558200359344482, 'learning_rate': 4.892823858341099e-07, 'epoch': 2.97}\n",
            "{'train_runtime': 5208.6709, 'train_samples_per_second': 316.415, 'train_steps_per_second': 4.944, 'train_loss': 0.3633834380250275, 'epoch': 3.0}\n",
            "100% 25752/25752 [1:26:47<00:00,  4.95it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./trained_model/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/yqwlbq1b\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_033405-yqwlbq1b/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPMGbfG6DAUN",
        "outputId": "71406741-8be0-4064-a2d2-75bd81a55b50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 05:01:44.261561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 05:01:44.282296: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 05:01:44.288465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 05:01:44.303590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 05:01:45.375530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "100% 1231/1231 [00:16<00:00, 74.78it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_050214-9xr3nih2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./eval_output/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/9xr3nih2\u001b[0m\n",
            "100% 1231/1231 [00:17<00:00, 68.59it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.30133700370788574, 'eval_model_preparation_time': 0.0038, 'eval_accuracy': 0.8984962701797485, 'eval_runtime': 17.1953, 'eval_samples_per_second': 572.365, 'eval_steps_per_second': 71.589}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./eval_output/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/9xr3nih2\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_050214-9xr3nih2/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --per_device_train_batch_size 64 --output_dir ./trained_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtVyBPy2DH19",
        "outputId": "a0ca3b73-73f1-4c54-dfe3-ddba991bd82a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 05:06:48.648429: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 05:06:48.668621: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 05:06:48.674547: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 05:06:48.689009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 05:06:49.743069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Filter: 100% 9824/9824 [00:00<00:00, 73565.23 examples/s]\n",
            "Filter: 100% 9842/9842 [00:00<00:00, 78250.34 examples/s]\n",
            "Filter: 100% 549367/549367 [00:06<00:00, 80878.78 examples/s]\n",
            "Map (num_proc=2): 100% 366603/366603 [00:34<00:00, 10564.23 examples/s]\n",
            "/content/run.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_050745-9cbvu8in\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./trained_model/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/9cbvu8in\u001b[0m\n",
            "{'loss': 0.3901, 'grad_norm': 2.3249008655548096, 'learning_rate': 4.854541223017397e-05, 'epoch': 0.09}\n",
            "{'loss': 0.221, 'grad_norm': 2.978034496307373, 'learning_rate': 4.709082446034794e-05, 'epoch': 0.17}\n",
            "{'loss': 0.1893, 'grad_norm': 3.1159820556640625, 'learning_rate': 4.563623669052191e-05, 'epoch': 0.26}\n",
            "{'loss': 0.1793, 'grad_norm': 2.3015923500061035, 'learning_rate': 4.418164892069588e-05, 'epoch': 0.35}\n",
            "{'loss': 0.1744, 'grad_norm': 5.194055557250977, 'learning_rate': 4.272706115086985e-05, 'epoch': 0.44}\n",
            "{'loss': 0.1649, 'grad_norm': 3.467217206954956, 'learning_rate': 4.127247338104382e-05, 'epoch': 0.52}\n",
            "{'loss': 0.1596, 'grad_norm': 3.6546101570129395, 'learning_rate': 3.981788561121778e-05, 'epoch': 0.61}\n",
            "{'loss': 0.1526, 'grad_norm': 2.116358518600464, 'learning_rate': 3.8363297841391746e-05, 'epoch': 0.7}\n",
            "{'loss': 0.1543, 'grad_norm': 5.979589462280273, 'learning_rate': 3.690871007156572e-05, 'epoch': 0.79}\n",
            "{'loss': 0.1441, 'grad_norm': 3.305643320083618, 'learning_rate': 3.545412230173969e-05, 'epoch': 0.87}\n",
            "{'loss': 0.1433, 'grad_norm': 4.940638542175293, 'learning_rate': 3.399953453191366e-05, 'epoch': 0.96}\n",
            "{'loss': 0.1274, 'grad_norm': 3.5172958374023438, 'learning_rate': 3.2544946762087625e-05, 'epoch': 1.05}\n",
            "{'loss': 0.1161, 'grad_norm': 1.9845035076141357, 'learning_rate': 3.109035899226159e-05, 'epoch': 1.13}\n",
            "{'loss': 0.1174, 'grad_norm': 4.442981243133545, 'learning_rate': 2.9635771222435564e-05, 'epoch': 1.22}\n",
            "{'loss': 0.1115, 'grad_norm': 1.1946543455123901, 'learning_rate': 2.8181183452609532e-05, 'epoch': 1.31}\n",
            "{'loss': 0.1152, 'grad_norm': 2.1085264682769775, 'learning_rate': 2.67265956827835e-05, 'epoch': 1.4}\n",
            "{'loss': 0.1163, 'grad_norm': 3.7252461910247803, 'learning_rate': 2.5272007912957468e-05, 'epoch': 1.48}\n",
            "{'loss': 0.1105, 'grad_norm': 4.205595016479492, 'learning_rate': 2.381742014313144e-05, 'epoch': 1.57}\n",
            "{'loss': 0.1069, 'grad_norm': 1.10564124584198, 'learning_rate': 2.2362832373305407e-05, 'epoch': 1.66}\n",
            "{'loss': 0.1093, 'grad_norm': 1.4003297090530396, 'learning_rate': 2.0908244603479375e-05, 'epoch': 1.75}\n",
            "{'loss': 0.1137, 'grad_norm': 3.704219102859497, 'learning_rate': 1.9453656833653343e-05, 'epoch': 1.83}\n",
            "{'loss': 0.1057, 'grad_norm': 4.027830123901367, 'learning_rate': 1.799906906382731e-05, 'epoch': 1.92}\n",
            "{'loss': 0.1014, 'grad_norm': 2.423740863800049, 'learning_rate': 1.6544481294001282e-05, 'epoch': 2.01}\n",
            "{'loss': 0.0825, 'grad_norm': 4.708984851837158, 'learning_rate': 1.508989352417525e-05, 'epoch': 2.09}\n",
            "{'loss': 0.0846, 'grad_norm': 2.201603889465332, 'learning_rate': 1.3635305754349218e-05, 'epoch': 2.18}\n",
            "{'loss': 0.0815, 'grad_norm': 4.802207946777344, 'learning_rate': 1.2180717984523186e-05, 'epoch': 2.27}\n",
            "{'loss': 0.0852, 'grad_norm': 1.25143563747406, 'learning_rate': 1.0726130214697156e-05, 'epoch': 2.36}\n",
            "{'loss': 0.0842, 'grad_norm': 2.9028079509735107, 'learning_rate': 9.271542444871124e-06, 'epoch': 2.44}\n",
            "{'loss': 0.085, 'grad_norm': 2.0735974311828613, 'learning_rate': 7.816954675045092e-06, 'epoch': 2.53}\n",
            "{'loss': 0.0833, 'grad_norm': 3.457274913787842, 'learning_rate': 6.362366905219061e-06, 'epoch': 2.62}\n",
            "{'loss': 0.083, 'grad_norm': 3.876706123352051, 'learning_rate': 4.90777913539303e-06, 'epoch': 2.71}\n",
            "{'loss': 0.0805, 'grad_norm': 2.6724767684936523, 'learning_rate': 3.453191365566998e-06, 'epoch': 2.79}\n",
            "{'loss': 0.0828, 'grad_norm': 0.7995008230209351, 'learning_rate': 1.9986035957409673e-06, 'epoch': 2.88}\n",
            "{'loss': 0.082, 'grad_norm': 3.2942488193511963, 'learning_rate': 5.440158259149357e-07, 'epoch': 2.97}\n",
            "{'train_runtime': 3475.1317, 'train_samples_per_second': 316.48, 'train_steps_per_second': 4.946, 'train_loss': 0.12716445411251098, 'epoch': 3.0}\n",
            "100% 17187/17187 [57:53<00:00,  4.95it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./trained_model/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/9cbvu8in\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_050745-9cbvu8in/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq0fqtXpRzcj",
        "outputId": "7536aef8-253e-4185-e071-9ca255146c49"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 06:06:17.106784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 06:06:17.128647: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 06:06:17.134902: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 06:06:17.150731: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 06:06:18.212488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "100% 1227/1231 [00:16<00:00, 73.63it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_060647-a4dgaj2h\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./eval_output/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/a4dgaj2h\u001b[0m\n",
            "100% 1231/1231 [00:18<00:00, 68.07it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 4.530858039855957, 'eval_model_preparation_time': 0.0032, 'eval_accuracy': 0.6477342247962952, 'eval_runtime': 17.2211, 'eval_samples_per_second': 571.507, 'eval_steps_per_second': 71.482}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./eval_output/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/a4dgaj2h\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_060647-a4dgaj2h/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pBhAhNmloGVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --per_device_train_batch_size 32 --output_dir ./trained_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBbYHIkzSac-",
        "outputId": "a008cbea-d0f5-4474-e9b3-8bde6837cf85"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 06:10:28.845567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 06:10:28.865655: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 06:10:28.871600: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 06:10:28.885920: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 06:10:29.918068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:160: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_061042-e3uo2n4s\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./trained_model/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/e3uo2n4s\u001b[0m\n",
            "{'loss': 0.7835, 'grad_norm': 9.505208969116211, 'learning_rate': 4.951460080770426e-05, 'epoch': 0.03}\n",
            "{'loss': 0.5946, 'grad_norm': 8.496546745300293, 'learning_rate': 4.902920161540851e-05, 'epoch': 0.06}\n",
            "{'loss': 0.5452, 'grad_norm': 5.7682647705078125, 'learning_rate': 4.854380242311277e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5343, 'grad_norm': 4.914712429046631, 'learning_rate': 4.805840323081703e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5064, 'grad_norm': 10.611634254455566, 'learning_rate': 4.757300403852128e-05, 'epoch': 0.15}\n",
            "{'loss': 0.4956, 'grad_norm': 6.0367817878723145, 'learning_rate': 4.708760484622554e-05, 'epoch': 0.17}\n",
            "{'loss': 0.4826, 'grad_norm': 10.455060005187988, 'learning_rate': 4.660220565392979e-05, 'epoch': 0.2}\n",
            "{'loss': 0.4685, 'grad_norm': 7.361387252807617, 'learning_rate': 4.611680646163405e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4614, 'grad_norm': 9.837108612060547, 'learning_rate': 4.563140726933831e-05, 'epoch': 0.26}\n",
            "{'loss': 0.4599, 'grad_norm': 5.290635108947754, 'learning_rate': 4.514600807704256e-05, 'epoch': 0.29}\n",
            "{'loss': 0.4583, 'grad_norm': 6.082979679107666, 'learning_rate': 4.466060888474682e-05, 'epoch': 0.32}\n",
            "{'loss': 0.4447, 'grad_norm': 6.433116912841797, 'learning_rate': 4.4175209692451076e-05, 'epoch': 0.35}\n",
            "{'loss': 0.4345, 'grad_norm': 5.851627826690674, 'learning_rate': 4.3689810500155334e-05, 'epoch': 0.38}\n",
            "{'loss': 0.438, 'grad_norm': 7.049382209777832, 'learning_rate': 4.3204411307859586e-05, 'epoch': 0.41}\n",
            "{'loss': 0.4382, 'grad_norm': 2.7768921852111816, 'learning_rate': 4.271901211556384e-05, 'epoch': 0.44}\n",
            "{'loss': 0.436, 'grad_norm': 6.4907402992248535, 'learning_rate': 4.2233612923268096e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4186, 'grad_norm': 5.1188459396362305, 'learning_rate': 4.1748213730972354e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4234, 'grad_norm': 6.671215534210205, 'learning_rate': 4.1262814538676606e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4263, 'grad_norm': 5.443106174468994, 'learning_rate': 4.0777415346380864e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4277, 'grad_norm': 3.058875799179077, 'learning_rate': 4.029201615408512e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4155, 'grad_norm': 4.0442681312561035, 'learning_rate': 3.980661696178938e-05, 'epoch': 0.61}\n",
            "{'loss': 0.4189, 'grad_norm': 4.8207783699035645, 'learning_rate': 3.932121776949363e-05, 'epoch': 0.64}\n",
            "{'loss': 0.4097, 'grad_norm': 4.338125228881836, 'learning_rate': 3.883581857719789e-05, 'epoch': 0.67}\n",
            "{'loss': 0.4086, 'grad_norm': 5.172999382019043, 'learning_rate': 3.835041938490215e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4085, 'grad_norm': 5.33972692489624, 'learning_rate': 3.78650201926064e-05, 'epoch': 0.73}\n",
            "{'loss': 0.4151, 'grad_norm': 6.594069480895996, 'learning_rate': 3.737962100031065e-05, 'epoch': 0.76}\n",
            "{'loss': 0.3964, 'grad_norm': 4.815063953399658, 'learning_rate': 3.689422180801491e-05, 'epoch': 0.79}\n",
            "{'loss': 0.3941, 'grad_norm': 3.516993999481201, 'learning_rate': 3.640882261571917e-05, 'epoch': 0.82}\n",
            "{'loss': 0.396, 'grad_norm': 7.250895977020264, 'learning_rate': 3.592342342342343e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4064, 'grad_norm': 4.810301780700684, 'learning_rate': 3.543802423112768e-05, 'epoch': 0.87}\n",
            "{'loss': 0.3933, 'grad_norm': 4.157702445983887, 'learning_rate': 3.495262503883194e-05, 'epoch': 0.9}\n",
            "{'loss': 0.3973, 'grad_norm': 4.945472717285156, 'learning_rate': 3.4467225846536196e-05, 'epoch': 0.93}\n",
            "{'loss': 0.3931, 'grad_norm': 6.552722454071045, 'learning_rate': 3.3981826654240454e-05, 'epoch': 0.96}\n",
            "{'loss': 0.3843, 'grad_norm': 6.74968147277832, 'learning_rate': 3.3496427461944706e-05, 'epoch': 0.99}\n",
            "{'loss': 0.3621, 'grad_norm': 8.160603523254395, 'learning_rate': 3.301102826964896e-05, 'epoch': 1.02}\n",
            "{'loss': 0.3538, 'grad_norm': 4.086616039276123, 'learning_rate': 3.2525629077353216e-05, 'epoch': 1.05}\n",
            "{'loss': 0.3422, 'grad_norm': 5.418557167053223, 'learning_rate': 3.2040229885057474e-05, 'epoch': 1.08}\n",
            "{'loss': 0.3535, 'grad_norm': 5.692623138427734, 'learning_rate': 3.1554830692761726e-05, 'epoch': 1.11}\n",
            "{'loss': 0.3502, 'grad_norm': 8.346089363098145, 'learning_rate': 3.1069431500465984e-05, 'epoch': 1.14}\n",
            "{'loss': 0.3602, 'grad_norm': 8.097162246704102, 'learning_rate': 3.058403230817024e-05, 'epoch': 1.16}\n",
            "{'loss': 0.3594, 'grad_norm': 6.6621623039245605, 'learning_rate': 3.0098633115874497e-05, 'epoch': 1.19}\n",
            "{'loss': 0.3485, 'grad_norm': 4.8059916496276855, 'learning_rate': 2.9613233923578752e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3459, 'grad_norm': 6.50543737411499, 'learning_rate': 2.912783473128301e-05, 'epoch': 1.25}\n",
            "{'loss': 0.3397, 'grad_norm': 5.755363941192627, 'learning_rate': 2.8642435538987266e-05, 'epoch': 1.28}\n",
            "{'loss': 0.3489, 'grad_norm': 5.275498867034912, 'learning_rate': 2.8157036346691517e-05, 'epoch': 1.31}\n",
            "{'loss': 0.3472, 'grad_norm': 4.822111129760742, 'learning_rate': 2.7671637154395776e-05, 'epoch': 1.34}\n",
            "{'loss': 0.344, 'grad_norm': 4.734896659851074, 'learning_rate': 2.718623796210003e-05, 'epoch': 1.37}\n",
            "{'loss': 0.3446, 'grad_norm': 6.024232864379883, 'learning_rate': 2.670083876980429e-05, 'epoch': 1.4}\n",
            "{'loss': 0.3425, 'grad_norm': 6.0122761726379395, 'learning_rate': 2.6215439577508544e-05, 'epoch': 1.43}\n",
            "{'loss': 0.341, 'grad_norm': 5.296990394592285, 'learning_rate': 2.57300403852128e-05, 'epoch': 1.46}\n",
            "{'loss': 0.3326, 'grad_norm': 8.239192008972168, 'learning_rate': 2.5244641192917057e-05, 'epoch': 1.49}\n",
            "{'loss': 0.3504, 'grad_norm': 7.175638198852539, 'learning_rate': 2.4759242000621312e-05, 'epoch': 1.51}\n",
            "{'loss': 0.3404, 'grad_norm': 4.067080020904541, 'learning_rate': 2.4273842808325567e-05, 'epoch': 1.54}\n",
            "{'loss': 0.3394, 'grad_norm': 4.09866189956665, 'learning_rate': 2.3788443616029822e-05, 'epoch': 1.57}\n",
            "{'loss': 0.3406, 'grad_norm': 4.111072063446045, 'learning_rate': 2.330304442373408e-05, 'epoch': 1.6}\n",
            "{'loss': 0.3451, 'grad_norm': 4.5837883949279785, 'learning_rate': 2.2817645231438335e-05, 'epoch': 1.63}\n",
            "{'loss': 0.3394, 'grad_norm': 5.563645362854004, 'learning_rate': 2.2332246039142594e-05, 'epoch': 1.66}\n",
            "{'loss': 0.3367, 'grad_norm': 6.986840724945068, 'learning_rate': 2.1846846846846845e-05, 'epoch': 1.69}\n",
            "{'loss': 0.339, 'grad_norm': 4.301964282989502, 'learning_rate': 2.1361447654551104e-05, 'epoch': 1.72}\n",
            "{'loss': 0.3398, 'grad_norm': 3.866783618927002, 'learning_rate': 2.087604846225536e-05, 'epoch': 1.75}\n",
            "{'loss': 0.3337, 'grad_norm': 5.49829626083374, 'learning_rate': 2.0390649269959617e-05, 'epoch': 1.78}\n",
            "{'loss': 0.3341, 'grad_norm': 3.852581739425659, 'learning_rate': 1.9905250077663872e-05, 'epoch': 1.81}\n",
            "{'loss': 0.3352, 'grad_norm': 5.130709171295166, 'learning_rate': 1.9419850885368127e-05, 'epoch': 1.83}\n",
            "{'loss': 0.3298, 'grad_norm': 5.304854869842529, 'learning_rate': 1.8934451693072382e-05, 'epoch': 1.86}\n",
            "{'loss': 0.3339, 'grad_norm': 4.267237186431885, 'learning_rate': 1.844905250077664e-05, 'epoch': 1.89}\n",
            "{'loss': 0.3299, 'grad_norm': 4.614767074584961, 'learning_rate': 1.7963653308480895e-05, 'epoch': 1.92}\n",
            "{'loss': 0.3259, 'grad_norm': 2.2976796627044678, 'learning_rate': 1.7478254116185154e-05, 'epoch': 1.95}\n",
            "{'loss': 0.3391, 'grad_norm': 6.014950275421143, 'learning_rate': 1.6992854923889405e-05, 'epoch': 1.98}\n",
            "{'loss': 0.3101, 'grad_norm': 5.5093913078308105, 'learning_rate': 1.6507455731593664e-05, 'epoch': 2.01}\n",
            "{'loss': 0.2959, 'grad_norm': 9.125600814819336, 'learning_rate': 1.602205653929792e-05, 'epoch': 2.04}\n",
            "{'loss': 0.2828, 'grad_norm': 8.798918724060059, 'learning_rate': 1.5536657347002177e-05, 'epoch': 2.07}\n",
            "{'loss': 0.2811, 'grad_norm': 7.079564571380615, 'learning_rate': 1.5051258154706432e-05, 'epoch': 2.1}\n",
            "{'loss': 0.294, 'grad_norm': 4.701143741607666, 'learning_rate': 1.4565858962410685e-05, 'epoch': 2.13}\n",
            "{'loss': 0.2995, 'grad_norm': 7.430436611175537, 'learning_rate': 1.4080459770114942e-05, 'epoch': 2.16}\n",
            "{'loss': 0.2911, 'grad_norm': 5.638437747955322, 'learning_rate': 1.3595060577819199e-05, 'epoch': 2.18}\n",
            "{'loss': 0.2956, 'grad_norm': 3.22139835357666, 'learning_rate': 1.3109661385523455e-05, 'epoch': 2.21}\n",
            "{'loss': 0.3043, 'grad_norm': 7.659285068511963, 'learning_rate': 1.2624262193227712e-05, 'epoch': 2.24}\n",
            "{'loss': 0.2941, 'grad_norm': 7.101765155792236, 'learning_rate': 1.2138863000931967e-05, 'epoch': 2.27}\n",
            "{'loss': 0.2915, 'grad_norm': 5.229753494262695, 'learning_rate': 1.1653463808636222e-05, 'epoch': 2.3}\n",
            "{'loss': 0.2923, 'grad_norm': 7.005486488342285, 'learning_rate': 1.1168064616340479e-05, 'epoch': 2.33}\n",
            "{'loss': 0.2837, 'grad_norm': 5.021901607513428, 'learning_rate': 1.0682665424044735e-05, 'epoch': 2.36}\n",
            "{'loss': 0.2972, 'grad_norm': 4.893861293792725, 'learning_rate': 1.019726623174899e-05, 'epoch': 2.39}\n",
            "{'loss': 0.2888, 'grad_norm': 3.7251222133636475, 'learning_rate': 9.711867039453247e-06, 'epoch': 2.42}\n",
            "{'loss': 0.2904, 'grad_norm': 6.470163345336914, 'learning_rate': 9.226467847157502e-06, 'epoch': 2.45}\n",
            "{'loss': 0.287, 'grad_norm': 8.646526336669922, 'learning_rate': 8.741068654861759e-06, 'epoch': 2.48}\n",
            "{'loss': 0.2889, 'grad_norm': 4.743546962738037, 'learning_rate': 8.255669462566015e-06, 'epoch': 2.5}\n",
            "{'loss': 0.2901, 'grad_norm': 8.939729690551758, 'learning_rate': 7.77027027027027e-06, 'epoch': 2.53}\n",
            "{'loss': 0.2882, 'grad_norm': 5.011160850524902, 'learning_rate': 7.284871077974527e-06, 'epoch': 2.56}\n",
            "{'loss': 0.2887, 'grad_norm': 4.956212997436523, 'learning_rate': 6.799471885678782e-06, 'epoch': 2.59}\n",
            "{'loss': 0.2887, 'grad_norm': 5.169075012207031, 'learning_rate': 6.314072693383039e-06, 'epoch': 2.62}\n",
            "{'loss': 0.2873, 'grad_norm': 2.99257755279541, 'learning_rate': 5.8286735010872945e-06, 'epoch': 2.65}\n",
            "{'loss': 0.2931, 'grad_norm': 4.021177291870117, 'learning_rate': 5.34327430879155e-06, 'epoch': 2.68}\n",
            "{'loss': 0.2784, 'grad_norm': 5.778072357177734, 'learning_rate': 4.857875116495807e-06, 'epoch': 2.71}\n",
            "{'loss': 0.2811, 'grad_norm': 6.795976161956787, 'learning_rate': 4.372475924200063e-06, 'epoch': 2.74}\n",
            "{'loss': 0.279, 'grad_norm': 9.547738075256348, 'learning_rate': 3.887076731904319e-06, 'epoch': 2.77}\n",
            "{'loss': 0.2917, 'grad_norm': 5.451245307922363, 'learning_rate': 3.401677539608574e-06, 'epoch': 2.8}\n",
            "{'loss': 0.2923, 'grad_norm': 5.59616756439209, 'learning_rate': 2.9162783473128303e-06, 'epoch': 2.83}\n",
            "{'loss': 0.2865, 'grad_norm': 4.565292835235596, 'learning_rate': 2.430879155017086e-06, 'epoch': 2.85}\n",
            "{'loss': 0.2855, 'grad_norm': 6.130434989929199, 'learning_rate': 1.9454799627213424e-06, 'epoch': 2.88}\n",
            "{'loss': 0.2902, 'grad_norm': 6.005040168762207, 'learning_rate': 1.4600807704255982e-06, 'epoch': 2.91}\n",
            "{'loss': 0.2872, 'grad_norm': 4.732358455657959, 'learning_rate': 9.74681578129854e-07, 'epoch': 2.94}\n",
            "{'loss': 0.2855, 'grad_norm': 6.209756374359131, 'learning_rate': 4.892823858341099e-07, 'epoch': 2.97}\n",
            "{'loss': 0.2875, 'grad_norm': 10.30097770690918, 'learning_rate': 3.8831935383659524e-09, 'epoch': 3.0}\n",
            "{'train_runtime': 5551.1254, 'train_samples_per_second': 296.895, 'train_steps_per_second': 9.278, 'train_loss': 0.360315060591931, 'epoch': 3.0}\n",
            "100% 51504/51504 [1:32:29<00:00,  9.28it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./trained_model/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/e3uo2n4s\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_061042-e3uo2n4s/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcIXRbasoH8z",
        "outputId": "3ac5cb2c-ca3a-482b-b603-f01562cc11a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 07:44:19.742977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 07:44:19.763483: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 07:44:19.769694: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 07:44:19.784170: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 07:44:20.864095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:160: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "100% 1225/1231 [00:17<00:00, 72.49it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_074448-guiw0up4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./eval_output/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/guiw0up4\u001b[0m\n",
            "100% 1231/1231 [00:18<00:00, 66.08it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.29912227392196655, 'eval_model_preparation_time': 0.0033, 'eval_accuracy': 0.8995122909545898, 'eval_runtime': 17.8387, 'eval_samples_per_second': 551.721, 'eval_steps_per_second': 69.007}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./eval_output/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/guiw0up4\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_074448-guiw0up4/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NPeLCLKRSyut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --per_device_train_batch_size 16 --output_dir ./trained_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4hRRzCdpUWn",
        "outputId": "f7028e05-3a95-4790-e678-634db39bbcf1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 07:49:21.165490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 07:49:21.186402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 07:49:21.192609: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 07:49:21.207894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 07:49:22.281188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:160: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_074934-r59fzgsk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./trained_model/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/r59fzgsk\u001b[0m\n",
            "{'loss': 0.8328, 'grad_norm': 9.781692504882812, 'learning_rate': 4.975730040385213e-05, 'epoch': 0.01}\n",
            "{'loss': 0.6635, 'grad_norm': 8.858389854431152, 'learning_rate': 4.951460080770426e-05, 'epoch': 0.03}\n",
            "{'loss': 0.6077, 'grad_norm': 8.281064987182617, 'learning_rate': 4.9271901211556387e-05, 'epoch': 0.04}\n",
            "{'loss': 0.5821, 'grad_norm': 8.164152145385742, 'learning_rate': 4.902920161540851e-05, 'epoch': 0.06}\n",
            "{'loss': 0.565, 'grad_norm': 10.429177284240723, 'learning_rate': 4.8786502019260645e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5456, 'grad_norm': 11.36184024810791, 'learning_rate': 4.854380242311277e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5345, 'grad_norm': 9.670112609863281, 'learning_rate': 4.83011028269649e-05, 'epoch': 0.1}\n",
            "{'loss': 0.5479, 'grad_norm': 6.818374156951904, 'learning_rate': 4.805840323081703e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5108, 'grad_norm': 15.639196395874023, 'learning_rate': 4.7815703634669155e-05, 'epoch': 0.13}\n",
            "{'loss': 0.5148, 'grad_norm': 9.85551929473877, 'learning_rate': 4.757300403852128e-05, 'epoch': 0.15}\n",
            "{'loss': 0.4991, 'grad_norm': 6.069068908691406, 'learning_rate': 4.7330304442373406e-05, 'epoch': 0.16}\n",
            "{'loss': 0.5061, 'grad_norm': 5.264057636260986, 'learning_rate': 4.708760484622554e-05, 'epoch': 0.17}\n",
            "{'loss': 0.4998, 'grad_norm': 5.330458641052246, 'learning_rate': 4.6844905250077665e-05, 'epoch': 0.19}\n",
            "{'loss': 0.4772, 'grad_norm': 2.8709845542907715, 'learning_rate': 4.660220565392979e-05, 'epoch': 0.2}\n",
            "{'loss': 0.483, 'grad_norm': 9.240265846252441, 'learning_rate': 4.635950605778192e-05, 'epoch': 0.22}\n",
            "{'loss': 0.4773, 'grad_norm': 8.172136306762695, 'learning_rate': 4.611680646163405e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4673, 'grad_norm': 9.712628364562988, 'learning_rate': 4.5874106865486175e-05, 'epoch': 0.25}\n",
            "{'loss': 0.4711, 'grad_norm': 9.093478202819824, 'learning_rate': 4.563140726933831e-05, 'epoch': 0.26}\n",
            "{'loss': 0.4672, 'grad_norm': 9.863351821899414, 'learning_rate': 4.538870767319043e-05, 'epoch': 0.28}\n",
            "{'loss': 0.4701, 'grad_norm': 3.8864591121673584, 'learning_rate': 4.514600807704256e-05, 'epoch': 0.29}\n",
            "{'loss': 0.4748, 'grad_norm': 8.597878456115723, 'learning_rate': 4.490330848089469e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4598, 'grad_norm': 6.182747840881348, 'learning_rate': 4.466060888474682e-05, 'epoch': 0.32}\n",
            "{'loss': 0.4538, 'grad_norm': 5.126110076904297, 'learning_rate': 4.441790928859895e-05, 'epoch': 0.33}\n",
            "{'loss': 0.4503, 'grad_norm': 8.51037883758545, 'learning_rate': 4.4175209692451076e-05, 'epoch': 0.35}\n",
            "{'loss': 0.4556, 'grad_norm': 5.3621416091918945, 'learning_rate': 4.39325100963032e-05, 'epoch': 0.36}\n",
            "{'loss': 0.4307, 'grad_norm': 11.639198303222656, 'learning_rate': 4.3689810500155334e-05, 'epoch': 0.38}\n",
            "{'loss': 0.4559, 'grad_norm': 9.081921577453613, 'learning_rate': 4.344711090400746e-05, 'epoch': 0.39}\n",
            "{'loss': 0.4477, 'grad_norm': 8.237323760986328, 'learning_rate': 4.3204411307859586e-05, 'epoch': 0.41}\n",
            "{'loss': 0.4532, 'grad_norm': 8.47653865814209, 'learning_rate': 4.296171171171172e-05, 'epoch': 0.42}\n",
            "{'loss': 0.4451, 'grad_norm': 4.510329246520996, 'learning_rate': 4.271901211556384e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4316, 'grad_norm': 7.4271063804626465, 'learning_rate': 4.247631251941597e-05, 'epoch': 0.45}\n",
            "{'loss': 0.4489, 'grad_norm': 6.714901447296143, 'learning_rate': 4.2233612923268096e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4342, 'grad_norm': 9.741260528564453, 'learning_rate': 4.199091332712022e-05, 'epoch': 0.48}\n",
            "{'loss': 0.4196, 'grad_norm': 8.469328880310059, 'learning_rate': 4.1748213730972354e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4266, 'grad_norm': 14.606327056884766, 'learning_rate': 4.150551413482448e-05, 'epoch': 0.51}\n",
            "{'loss': 0.4436, 'grad_norm': 10.113234519958496, 'learning_rate': 4.1262814538676606e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4452, 'grad_norm': 10.989956855773926, 'learning_rate': 4.102011494252874e-05, 'epoch': 0.54}\n",
            "{'loss': 0.4301, 'grad_norm': 6.631522178649902, 'learning_rate': 4.0777415346380864e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4434, 'grad_norm': 4.611295223236084, 'learning_rate': 4.0534715750232997e-05, 'epoch': 0.57}\n",
            "{'loss': 0.4263, 'grad_norm': 6.604686260223389, 'learning_rate': 4.029201615408512e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4224, 'grad_norm': 9.597137451171875, 'learning_rate': 4.004931655793725e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4361, 'grad_norm': 4.339785099029541, 'learning_rate': 3.980661696178938e-05, 'epoch': 0.61}\n",
            "{'loss': 0.4307, 'grad_norm': 8.628564834594727, 'learning_rate': 3.9563917365641506e-05, 'epoch': 0.63}\n",
            "{'loss': 0.42, 'grad_norm': 6.037920951843262, 'learning_rate': 3.932121776949363e-05, 'epoch': 0.64}\n",
            "{'loss': 0.4301, 'grad_norm': 17.395231246948242, 'learning_rate': 3.9078518173345765e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4223, 'grad_norm': 6.317768573760986, 'learning_rate': 3.883581857719789e-05, 'epoch': 0.67}\n",
            "{'loss': 0.4281, 'grad_norm': 8.368631362915039, 'learning_rate': 3.8593118981050016e-05, 'epoch': 0.68}\n",
            "{'loss': 0.4143, 'grad_norm': 2.5417137145996094, 'learning_rate': 3.835041938490215e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4032, 'grad_norm': 8.599331855773926, 'learning_rate': 3.8107719788754275e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4278, 'grad_norm': 6.6688127517700195, 'learning_rate': 3.78650201926064e-05, 'epoch': 0.73}\n",
            "{'loss': 0.427, 'grad_norm': 7.3173346519470215, 'learning_rate': 3.7622320596458526e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4119, 'grad_norm': 3.8549015522003174, 'learning_rate': 3.737962100031065e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4083, 'grad_norm': 5.899850845336914, 'learning_rate': 3.7136921404162785e-05, 'epoch': 0.77}\n",
            "{'loss': 0.4055, 'grad_norm': 11.006568908691406, 'learning_rate': 3.689422180801491e-05, 'epoch': 0.79}\n",
            "{'loss': 0.4041, 'grad_norm': 9.656065940856934, 'learning_rate': 3.665152221186704e-05, 'epoch': 0.8}\n",
            "{'loss': 0.4095, 'grad_norm': 10.413159370422363, 'learning_rate': 3.640882261571917e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4168, 'grad_norm': 12.02453327178955, 'learning_rate': 3.6166123019571295e-05, 'epoch': 0.83}\n",
            "{'loss': 0.3977, 'grad_norm': 10.366836547851562, 'learning_rate': 3.592342342342343e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4194, 'grad_norm': 7.597317218780518, 'learning_rate': 3.568072382727555e-05, 'epoch': 0.86}\n",
            "{'loss': 0.403, 'grad_norm': 4.8129777908325195, 'learning_rate': 3.543802423112768e-05, 'epoch': 0.87}\n",
            "{'loss': 0.3967, 'grad_norm': 7.64404296875, 'learning_rate': 3.519532463497981e-05, 'epoch': 0.89}\n",
            "{'loss': 0.4082, 'grad_norm': 6.322027206420898, 'learning_rate': 3.495262503883194e-05, 'epoch': 0.9}\n",
            "{'loss': 0.4092, 'grad_norm': 7.108279705047607, 'learning_rate': 3.470992544268406e-05, 'epoch': 0.92}\n",
            "{'loss': 0.4098, 'grad_norm': 8.06596565246582, 'learning_rate': 3.4467225846536196e-05, 'epoch': 0.93}\n",
            "{'loss': 0.3992, 'grad_norm': 2.940014600753784, 'learning_rate': 3.422452625038832e-05, 'epoch': 0.95}\n",
            "{'loss': 0.4053, 'grad_norm': 4.410644054412842, 'learning_rate': 3.3981826654240454e-05, 'epoch': 0.96}\n",
            "{'loss': 0.4018, 'grad_norm': 4.749711513519287, 'learning_rate': 3.373912705809258e-05, 'epoch': 0.98}\n",
            "{'loss': 0.3892, 'grad_norm': 17.310178756713867, 'learning_rate': 3.3496427461944706e-05, 'epoch': 0.99}\n",
            "{'loss': 0.3909, 'grad_norm': 9.740436553955078, 'learning_rate': 3.325372786579684e-05, 'epoch': 1.0}\n",
            "{'loss': 0.3499, 'grad_norm': 14.304464340209961, 'learning_rate': 3.301102826964896e-05, 'epoch': 1.02}\n",
            "{'loss': 0.3519, 'grad_norm': 11.18731689453125, 'learning_rate': 3.276832867350109e-05, 'epoch': 1.03}\n",
            "{'loss': 0.36, 'grad_norm': 4.713994979858398, 'learning_rate': 3.2525629077353216e-05, 'epoch': 1.05}\n",
            "{'loss': 0.3464, 'grad_norm': 7.148744106292725, 'learning_rate': 3.228292948120534e-05, 'epoch': 1.06}\n",
            "{'loss': 0.3645, 'grad_norm': 4.792250633239746, 'learning_rate': 3.2040229885057474e-05, 'epoch': 1.08}\n",
            "{'loss': 0.3648, 'grad_norm': 10.144486427307129, 'learning_rate': 3.17975302889096e-05, 'epoch': 1.09}\n",
            "{'loss': 0.3665, 'grad_norm': 10.647272109985352, 'learning_rate': 3.1554830692761726e-05, 'epoch': 1.11}\n",
            "{'loss': 0.3579, 'grad_norm': 7.629514217376709, 'learning_rate': 3.131213109661386e-05, 'epoch': 1.12}\n",
            "{'loss': 0.3673, 'grad_norm': 11.092752456665039, 'learning_rate': 3.1069431500465984e-05, 'epoch': 1.14}\n",
            "{'loss': 0.357, 'grad_norm': 8.073381423950195, 'learning_rate': 3.082673190431811e-05, 'epoch': 1.15}\n",
            "{'loss': 0.3757, 'grad_norm': 12.547842979431152, 'learning_rate': 3.058403230817024e-05, 'epoch': 1.16}\n",
            "{'loss': 0.3693, 'grad_norm': 5.203790187835693, 'learning_rate': 3.0341332712022368e-05, 'epoch': 1.18}\n",
            "{'loss': 0.3601, 'grad_norm': 6.853420257568359, 'learning_rate': 3.0098633115874497e-05, 'epoch': 1.19}\n",
            "{'loss': 0.3531, 'grad_norm': 4.7772369384765625, 'learning_rate': 2.9855933519726626e-05, 'epoch': 1.21}\n",
            "{'loss': 0.3614, 'grad_norm': 5.9475250244140625, 'learning_rate': 2.9613233923578752e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3538, 'grad_norm': 5.559275150299072, 'learning_rate': 2.937053432743088e-05, 'epoch': 1.24}\n",
            "{'loss': 0.3539, 'grad_norm': 12.378275871276855, 'learning_rate': 2.912783473128301e-05, 'epoch': 1.25}\n",
            "{'loss': 0.3493, 'grad_norm': 5.103329658508301, 'learning_rate': 2.888513513513514e-05, 'epoch': 1.27}\n",
            "{'loss': 0.345, 'grad_norm': 5.723247528076172, 'learning_rate': 2.8642435538987266e-05, 'epoch': 1.28}\n",
            "{'loss': 0.3462, 'grad_norm': 7.247066974639893, 'learning_rate': 2.8399735942839395e-05, 'epoch': 1.3}\n",
            "{'loss': 0.37, 'grad_norm': 14.003838539123535, 'learning_rate': 2.8157036346691517e-05, 'epoch': 1.31}\n",
            "{'loss': 0.36, 'grad_norm': 11.39815616607666, 'learning_rate': 2.7914336750543646e-05, 'epoch': 1.33}\n",
            "{'loss': 0.3488, 'grad_norm': 4.440527439117432, 'learning_rate': 2.7671637154395776e-05, 'epoch': 1.34}\n",
            "{'loss': 0.3447, 'grad_norm': 5.90206241607666, 'learning_rate': 2.74289375582479e-05, 'epoch': 1.35}\n",
            "{'loss': 0.3632, 'grad_norm': 5.175408840179443, 'learning_rate': 2.718623796210003e-05, 'epoch': 1.37}\n",
            "{'loss': 0.3572, 'grad_norm': 4.950301170349121, 'learning_rate': 2.694353836595216e-05, 'epoch': 1.38}\n",
            "{'loss': 0.3537, 'grad_norm': 15.832294464111328, 'learning_rate': 2.670083876980429e-05, 'epoch': 1.4}\n",
            "{'loss': 0.3552, 'grad_norm': 8.690288543701172, 'learning_rate': 2.6458139173656415e-05, 'epoch': 1.41}\n",
            "{'loss': 0.3507, 'grad_norm': 7.881504535675049, 'learning_rate': 2.6215439577508544e-05, 'epoch': 1.43}\n",
            "{'loss': 0.3565, 'grad_norm': 6.225814342498779, 'learning_rate': 2.5972739981360673e-05, 'epoch': 1.44}\n",
            "{'loss': 0.3402, 'grad_norm': 7.30654764175415, 'learning_rate': 2.57300403852128e-05, 'epoch': 1.46}\n",
            "{'loss': 0.3422, 'grad_norm': 3.1993799209594727, 'learning_rate': 2.5487340789064928e-05, 'epoch': 1.47}\n",
            "{'loss': 0.3417, 'grad_norm': 9.142094612121582, 'learning_rate': 2.5244641192917057e-05, 'epoch': 1.49}\n",
            "{'loss': 0.3675, 'grad_norm': 7.26881217956543, 'learning_rate': 2.5001941596769186e-05, 'epoch': 1.5}\n",
            "{'loss': 0.3557, 'grad_norm': 5.212632656097412, 'learning_rate': 2.4759242000621312e-05, 'epoch': 1.51}\n",
            "{'loss': 0.3496, 'grad_norm': 5.034671306610107, 'learning_rate': 2.4516542404473438e-05, 'epoch': 1.53}\n",
            "{'loss': 0.3436, 'grad_norm': 9.390685081481934, 'learning_rate': 2.4273842808325567e-05, 'epoch': 1.54}\n",
            "{'loss': 0.3457, 'grad_norm': 3.6072769165039062, 'learning_rate': 2.4031143212177696e-05, 'epoch': 1.56}\n",
            "{'loss': 0.3591, 'grad_norm': 4.191600799560547, 'learning_rate': 2.3788443616029822e-05, 'epoch': 1.57}\n",
            "{'loss': 0.3494, 'grad_norm': 11.0828857421875, 'learning_rate': 2.354574401988195e-05, 'epoch': 1.59}\n",
            "{'loss': 0.3506, 'grad_norm': 16.906869888305664, 'learning_rate': 2.330304442373408e-05, 'epoch': 1.6}\n",
            "{'loss': 0.3595, 'grad_norm': 16.993389129638672, 'learning_rate': 2.306034482758621e-05, 'epoch': 1.62}\n",
            "{'loss': 0.3474, 'grad_norm': 7.0763421058654785, 'learning_rate': 2.2817645231438335e-05, 'epoch': 1.63}\n",
            "{'loss': 0.34, 'grad_norm': 13.891375541687012, 'learning_rate': 2.2574945635290465e-05, 'epoch': 1.65}\n",
            "{'loss': 0.3545, 'grad_norm': 10.504459381103516, 'learning_rate': 2.2332246039142594e-05, 'epoch': 1.66}\n",
            "{'loss': 0.3395, 'grad_norm': 9.15914535522461, 'learning_rate': 2.208954644299472e-05, 'epoch': 1.67}\n",
            "{'loss': 0.3562, 'grad_norm': 8.634602546691895, 'learning_rate': 2.1846846846846845e-05, 'epoch': 1.69}\n",
            "{'loss': 0.3416, 'grad_norm': 8.03852367401123, 'learning_rate': 2.1604147250698975e-05, 'epoch': 1.7}\n",
            "{'loss': 0.3484, 'grad_norm': 20.690343856811523, 'learning_rate': 2.1361447654551104e-05, 'epoch': 1.72}\n",
            "{'loss': 0.3557, 'grad_norm': 7.162750720977783, 'learning_rate': 2.1118748058403233e-05, 'epoch': 1.73}\n",
            "{'loss': 0.3383, 'grad_norm': 8.616992950439453, 'learning_rate': 2.087604846225536e-05, 'epoch': 1.75}\n",
            "{'loss': 0.3525, 'grad_norm': 5.361623287200928, 'learning_rate': 2.0633348866107488e-05, 'epoch': 1.76}\n",
            "{'loss': 0.3377, 'grad_norm': 3.057533025741577, 'learning_rate': 2.0390649269959617e-05, 'epoch': 1.78}\n",
            "{'loss': 0.3391, 'grad_norm': 6.095089912414551, 'learning_rate': 2.0147949673811743e-05, 'epoch': 1.79}\n",
            "{'loss': 0.3463, 'grad_norm': 6.792321681976318, 'learning_rate': 1.9905250077663872e-05, 'epoch': 1.81}\n",
            "{'loss': 0.3442, 'grad_norm': 7.699487209320068, 'learning_rate': 1.9662550481515998e-05, 'epoch': 1.82}\n",
            "{'loss': 0.3444, 'grad_norm': 6.363056659698486, 'learning_rate': 1.9419850885368127e-05, 'epoch': 1.83}\n",
            "{'loss': 0.3445, 'grad_norm': 12.855690956115723, 'learning_rate': 1.9177151289220256e-05, 'epoch': 1.85}\n",
            "{'loss': 0.3338, 'grad_norm': 31.9964599609375, 'learning_rate': 1.8934451693072382e-05, 'epoch': 1.86}\n",
            "{'loss': 0.3517, 'grad_norm': 5.5848493576049805, 'learning_rate': 1.869175209692451e-05, 'epoch': 1.88}\n",
            "{'loss': 0.3412, 'grad_norm': 4.286800384521484, 'learning_rate': 1.844905250077664e-05, 'epoch': 1.89}\n",
            "{'loss': 0.3382, 'grad_norm': 10.380217552185059, 'learning_rate': 1.8206352904628766e-05, 'epoch': 1.91}\n",
            "{'loss': 0.3268, 'grad_norm': 6.025358200073242, 'learning_rate': 1.7963653308480895e-05, 'epoch': 1.92}\n",
            "{'loss': 0.3367, 'grad_norm': 4.733659744262695, 'learning_rate': 1.7720953712333025e-05, 'epoch': 1.94}\n",
            "{'loss': 0.3314, 'grad_norm': 3.5855729579925537, 'learning_rate': 1.7478254116185154e-05, 'epoch': 1.95}\n",
            "{'loss': 0.3483, 'grad_norm': 9.137218475341797, 'learning_rate': 1.723555452003728e-05, 'epoch': 1.97}\n",
            "{'loss': 0.3339, 'grad_norm': 6.809654712677002, 'learning_rate': 1.6992854923889405e-05, 'epoch': 1.98}\n",
            "{'loss': 0.3271, 'grad_norm': 7.029824733734131, 'learning_rate': 1.6750155327741535e-05, 'epoch': 1.99}\n",
            "{'loss': 0.3177, 'grad_norm': 14.839651107788086, 'learning_rate': 1.6507455731593664e-05, 'epoch': 2.01}\n",
            "{'loss': 0.3079, 'grad_norm': 10.799310684204102, 'learning_rate': 1.626475613544579e-05, 'epoch': 2.02}\n",
            "{'loss': 0.2897, 'grad_norm': 8.785760879516602, 'learning_rate': 1.602205653929792e-05, 'epoch': 2.04}\n",
            "{'loss': 0.2958, 'grad_norm': 0.880275547504425, 'learning_rate': 1.5779356943150048e-05, 'epoch': 2.05}\n",
            "{'loss': 0.2918, 'grad_norm': 11.766439437866211, 'learning_rate': 1.5536657347002177e-05, 'epoch': 2.07}\n",
            "{'loss': 0.2908, 'grad_norm': 10.975578308105469, 'learning_rate': 1.5293957750854303e-05, 'epoch': 2.08}\n",
            "{'loss': 0.2925, 'grad_norm': 16.844051361083984, 'learning_rate': 1.5051258154706432e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3114, 'grad_norm': 7.142129421234131, 'learning_rate': 1.4808558558558558e-05, 'epoch': 2.11}\n",
            "{'loss': 0.2903, 'grad_norm': 3.019660711288452, 'learning_rate': 1.4565858962410685e-05, 'epoch': 2.13}\n",
            "{'loss': 0.3007, 'grad_norm': 6.7717742919921875, 'learning_rate': 1.4323159366262815e-05, 'epoch': 2.14}\n",
            "{'loss': 0.3229, 'grad_norm': 2.1187846660614014, 'learning_rate': 1.4080459770114942e-05, 'epoch': 2.16}\n",
            "{'loss': 0.2964, 'grad_norm': 0.9840574860572815, 'learning_rate': 1.3837760173967071e-05, 'epoch': 2.17}\n",
            "{'loss': 0.2946, 'grad_norm': 5.133413314819336, 'learning_rate': 1.3595060577819199e-05, 'epoch': 2.18}\n",
            "{'loss': 0.2929, 'grad_norm': 8.04159927368164, 'learning_rate': 1.3352360981671328e-05, 'epoch': 2.2}\n",
            "{'loss': 0.2932, 'grad_norm': 13.050291061401367, 'learning_rate': 1.3109661385523455e-05, 'epoch': 2.21}\n",
            "{'loss': 0.3041, 'grad_norm': 8.244548797607422, 'learning_rate': 1.2866961789375585e-05, 'epoch': 2.23}\n",
            "{'loss': 0.3059, 'grad_norm': 13.909466743469238, 'learning_rate': 1.2624262193227712e-05, 'epoch': 2.24}\n",
            "{'loss': 0.2921, 'grad_norm': 7.163558006286621, 'learning_rate': 1.238156259707984e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3073, 'grad_norm': 5.051223278045654, 'learning_rate': 1.2138863000931967e-05, 'epoch': 2.27}\n",
            "{'loss': 0.296, 'grad_norm': 12.356720924377441, 'learning_rate': 1.1896163404784096e-05, 'epoch': 2.29}\n",
            "{'loss': 0.3003, 'grad_norm': 5.28479528427124, 'learning_rate': 1.1653463808636222e-05, 'epoch': 2.3}\n",
            "{'loss': 0.2955, 'grad_norm': 2.510448455810547, 'learning_rate': 1.1410764212488351e-05, 'epoch': 2.32}\n",
            "{'loss': 0.3084, 'grad_norm': 8.742277145385742, 'learning_rate': 1.1168064616340479e-05, 'epoch': 2.33}\n",
            "{'loss': 0.2872, 'grad_norm': 6.08164119720459, 'learning_rate': 1.0925365020192608e-05, 'epoch': 2.34}\n",
            "{'loss': 0.2859, 'grad_norm': 8.409049034118652, 'learning_rate': 1.0682665424044735e-05, 'epoch': 2.36}\n",
            "{'loss': 0.2965, 'grad_norm': 1.412502646446228, 'learning_rate': 1.0439965827896863e-05, 'epoch': 2.37}\n",
            "{'loss': 0.3043, 'grad_norm': 2.6974430084228516, 'learning_rate': 1.019726623174899e-05, 'epoch': 2.39}\n",
            "{'loss': 0.3057, 'grad_norm': 3.486194133758545, 'learning_rate': 9.95456663560112e-06, 'epoch': 2.4}\n",
            "{'loss': 0.2917, 'grad_norm': 3.977067232131958, 'learning_rate': 9.711867039453247e-06, 'epoch': 2.42}\n",
            "{'loss': 0.3165, 'grad_norm': 12.323836326599121, 'learning_rate': 9.469167443305376e-06, 'epoch': 2.43}\n",
            "{'loss': 0.2839, 'grad_norm': 15.0245361328125, 'learning_rate': 9.226467847157502e-06, 'epoch': 2.45}\n",
            "{'loss': 0.2918, 'grad_norm': 0.730017364025116, 'learning_rate': 8.983768251009631e-06, 'epoch': 2.46}\n",
            "{'loss': 0.2928, 'grad_norm': 18.643726348876953, 'learning_rate': 8.741068654861759e-06, 'epoch': 2.48}\n",
            "{'loss': 0.2864, 'grad_norm': 14.823781967163086, 'learning_rate': 8.498369058713886e-06, 'epoch': 2.49}\n",
            "{'loss': 0.2976, 'grad_norm': 9.908895492553711, 'learning_rate': 8.255669462566015e-06, 'epoch': 2.5}\n",
            "{'loss': 0.3037, 'grad_norm': 14.41292953491211, 'learning_rate': 8.012969866418143e-06, 'epoch': 2.52}\n",
            "{'loss': 0.2932, 'grad_norm': 4.43940544128418, 'learning_rate': 7.77027027027027e-06, 'epoch': 2.53}\n",
            "{'loss': 0.2987, 'grad_norm': 3.660245418548584, 'learning_rate': 7.527570674122399e-06, 'epoch': 2.55}\n",
            "{'loss': 0.2953, 'grad_norm': 10.755057334899902, 'learning_rate': 7.284871077974527e-06, 'epoch': 2.56}\n",
            "{'loss': 0.2883, 'grad_norm': 6.0085344314575195, 'learning_rate': 7.042171481826655e-06, 'epoch': 2.58}\n",
            "{'loss': 0.2973, 'grad_norm': 12.290196418762207, 'learning_rate': 6.799471885678782e-06, 'epoch': 2.59}\n",
            "{'loss': 0.2971, 'grad_norm': 9.372218132019043, 'learning_rate': 6.55677228953091e-06, 'epoch': 2.61}\n",
            "{'loss': 0.2937, 'grad_norm': 14.857767105102539, 'learning_rate': 6.314072693383039e-06, 'epoch': 2.62}\n",
            "{'loss': 0.2923, 'grad_norm': 10.569852828979492, 'learning_rate': 6.071373097235167e-06, 'epoch': 2.64}\n",
            "{'loss': 0.2986, 'grad_norm': 12.083742141723633, 'learning_rate': 5.8286735010872945e-06, 'epoch': 2.65}\n",
            "{'loss': 0.2969, 'grad_norm': 1.659737229347229, 'learning_rate': 5.585973904939423e-06, 'epoch': 2.66}\n",
            "{'loss': 0.3002, 'grad_norm': 5.939831733703613, 'learning_rate': 5.34327430879155e-06, 'epoch': 2.68}\n",
            "{'loss': 0.2885, 'grad_norm': 7.147521495819092, 'learning_rate': 5.100574712643679e-06, 'epoch': 2.69}\n",
            "{'loss': 0.2804, 'grad_norm': 2.9778835773468018, 'learning_rate': 4.857875116495807e-06, 'epoch': 2.71}\n",
            "{'loss': 0.2878, 'grad_norm': 4.549549102783203, 'learning_rate': 4.6151755203479345e-06, 'epoch': 2.72}\n",
            "{'loss': 0.2825, 'grad_norm': 5.764304161071777, 'learning_rate': 4.372475924200063e-06, 'epoch': 2.74}\n",
            "{'loss': 0.2882, 'grad_norm': 8.688582420349121, 'learning_rate': 4.12977632805219e-06, 'epoch': 2.75}\n",
            "{'loss': 0.2847, 'grad_norm': 15.897294044494629, 'learning_rate': 3.887076731904319e-06, 'epoch': 2.77}\n",
            "{'loss': 0.2964, 'grad_norm': 6.257472515106201, 'learning_rate': 3.6443771357564465e-06, 'epoch': 2.78}\n",
            "{'loss': 0.2924, 'grad_norm': 5.352839946746826, 'learning_rate': 3.401677539608574e-06, 'epoch': 2.8}\n",
            "{'loss': 0.294, 'grad_norm': 7.008023262023926, 'learning_rate': 3.1589779434607024e-06, 'epoch': 2.81}\n",
            "{'loss': 0.294, 'grad_norm': 13.758646965026855, 'learning_rate': 2.9162783473128303e-06, 'epoch': 2.83}\n",
            "{'loss': 0.2925, 'grad_norm': 6.73029088973999, 'learning_rate': 2.673578751164958e-06, 'epoch': 2.84}\n",
            "{'loss': 0.3067, 'grad_norm': 5.889475345611572, 'learning_rate': 2.430879155017086e-06, 'epoch': 2.85}\n",
            "{'loss': 0.2903, 'grad_norm': 13.57536792755127, 'learning_rate': 2.188179558869214e-06, 'epoch': 2.87}\n",
            "{'loss': 0.2899, 'grad_norm': 9.53567886352539, 'learning_rate': 1.9454799627213424e-06, 'epoch': 2.88}\n",
            "{'loss': 0.2894, 'grad_norm': 15.117719650268555, 'learning_rate': 1.7027803665734703e-06, 'epoch': 2.9}\n",
            "{'loss': 0.29, 'grad_norm': 11.613332748413086, 'learning_rate': 1.4600807704255982e-06, 'epoch': 2.91}\n",
            "{'loss': 0.2877, 'grad_norm': 5.248271942138672, 'learning_rate': 1.217381174277726e-06, 'epoch': 2.93}\n",
            "{'loss': 0.2936, 'grad_norm': 11.549681663513184, 'learning_rate': 9.74681578129854e-07, 'epoch': 2.94}\n",
            "{'loss': 0.2928, 'grad_norm': 8.214789390563965, 'learning_rate': 7.31981981981982e-07, 'epoch': 2.96}\n",
            "{'loss': 0.29, 'grad_norm': 16.00908660888672, 'learning_rate': 4.892823858341099e-07, 'epoch': 2.97}\n",
            "{'loss': 0.2879, 'grad_norm': 14.73178768157959, 'learning_rate': 2.46582789686238e-07, 'epoch': 2.99}\n",
            "{'loss': 0.2868, 'grad_norm': 1.54673171043396, 'learning_rate': 3.8831935383659524e-09, 'epoch': 3.0}\n",
            "{'train_runtime': 6135.8672, 'train_samples_per_second': 268.601, 'train_steps_per_second': 16.788, 'train_loss': 0.36768846836435237, 'epoch': 3.0}\n",
            "100% 103008/103008 [1:42:14<00:00, 16.79it/s]\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./trained_model/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/r59fzgsk\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_074934-r59fzgsk/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRSpzTL4BIOH",
        "outputId": "2ef05efd-1636-41eb-f200-27e822f2815c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-14 09:33:19.505046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-14 09:33:19.525395: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-14 09:33:19.531495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-14 09:33:19.546152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-14 09:33:20.686142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "/content/run.py:160: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = trainer_class(\n",
            "100% 1226/1231 [00:16<00:00, 77.40it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskp4318\u001b[0m (\u001b[33mskp4318-utaustin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20241114_093348-1t21a09n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./eval_output/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/1t21a09n\u001b[0m\n",
            "100% 1231/1231 [00:17<00:00, 69.35it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.3128686547279358, 'eval_model_preparation_time': 0.0031, 'eval_accuracy': 0.8977850079536438, 'eval_runtime': 16.9378, 'eval_samples_per_second': 581.066, 'eval_steps_per_second': 72.678}\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./eval_output/\u001b[0m at: \u001b[34mhttps://wandb.ai/skp4318-utaustin/huggingface/runs/1t21a09n\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241114_093348-1t21a09n/logs\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNvfcKWhAIrpKW1+1bsqSMn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}